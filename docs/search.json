[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Assignment Blog",
    "section": "",
    "text": "Hello! This is my 1st Assignment.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nplot(iris)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello there! I am currently a graduate student in The University of Texas at Dallas’ Applied Cognition & Neuroscience Program, specializing in Human-Computer Interaction. With a background in psychology, I have always been interested in using my knowledge to support those in need. When I first learned about user experience (UX), I immediately fell in love with the field’s multidisciplinary nature and evidence-based process.\nI am passionate about creating digital services that are valuable and accessible to diverse user bases. I am a continuous learner and am excited to keep developing my skills through new opportunities in the future! Feel free to reach out and view my resume and/or portfolio.\nWhen I’m not working or studying, you can find me experimenting with songs on my keyboard, volunteering with friends, or exploring new worlds through literature."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Blog!",
    "section": "",
    "text": "Assignment 9\n\n\n\n\n\n\n\ntime-series\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 8\n\n\n\n\n\n\n\ndashboard\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 7\n\n\n\n\n\n\n\nr graphics\n\n\nggplot2\n\n\nexporting\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 6\n\n\n\n\n\n\n\nshiny\n\n\niframe\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 5\n\n\n\n\n\n\n\nr graphics\n\n\nggplot2\n\n\nexporting\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 4\n\n\n\n\n\n\n\ntidyverse\n\n\nr\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 3\n\n\n\n\n\n\n\ndata analysis\n\n\nbig data\n\n\noverfitting\n\n\noverparameterization\n\n\ntidyverse\n\n\nr\n\n\nscatterplot\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 3\n\n\n\n\n\n\n\ndata analysis\n\n\nbig data\n\n\noverfitting\n\n\noverparameterization\n\n\ntidyverse\n\n\nr\n\n\nscatterplot\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 2\n\n\n\n\n\n\n\ndata analysis\n\n\nr\n\n\ncleaning data\n\n\nscatterplot\n\n\nbarchart\n\n\nhistogram\n\n\nboxplot\n\n\nperspective chart\n\n\npie chart\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 1\n\n\n\n\n\n\n\ndata visualization\n\n\ngenerative art\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Assignment 1/index.html",
    "href": "posts/Assignment 1/index.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Hello! This assignment contains 4 sections:\n\nExamples of Generative Art\nA Lesson in Data Visualization: Critiquing a Chart\nExploring Ascombe.R\nChanging Colors on Fall.R\n\nI. Examples of Generative Art\nWhat is generative art? \nWhen I first think of this term, I immediately think of the use of artificial intelligence (AI) to create different pieces of art, drawing from other examples from the web or fed into some algorithm. Indeed, generative art often refers to pieces created by an algorithm, but to any autonomous system from various fields, such as statistics or life and physical sciences. Such systems operate as instructions, placing limits on how a work may turn out. However, different outcomes can be generated based on the amount of chance its creator introduces in applying the system. \nOne of the earliest examples of generative art includes George Nees’ Schotter (1968), as seen below. Nees used the ALGOL programming language and introduced random variables to create Schotter. While the top of the artwork begins as standard grid, the comprising squares begin to rotate and move, creating a gradient from order to chaos.  \n\n\n\n\n\n \nNowadays, generative art can be created using a greater variety of programming languages than was offered in 1968. For example, Javascript is one of the most popular languages utilized by artists today. Last spring, I took a web design class and identified several projects that could be classified as generative art. One piece in particular that stood out to me was DT Soulmate Matcher (1968), a piece created by then-MIT graduate student Munus Shih. The platform can be accessed here. A screenshot of the homepage can be found below. \n\n\n\n\n\nDT Soulmate Matcher was partly inspired by Shih’s experiences with classmates in his MFA program, who often wished they could meet more like-minded peers. Furthermore, Shih wanted to investigate the exploratory nature of data visualization and how it could be used to connect audiences. Collecting information from his peers, Shih created a system for how different traits would be visualized in the piece (I.e. patterns, distance between individual profiles) using the library p5.js. He also created an algorithm that allowed for students to be assigned a “soulmate,” or peer that they had most in common with. This system was coded in a way that it could accommodate for future student data, thus altering the initial displays and matches of the page.  \nAnother example of generative art using p5.js includes the following image, created by Ike Stevens. While it may seem the piece might initially appear random at first, it is actually created using Oscar Best Picture Nomination data. The entire process can be further explained in this Medium article, Visualized Best Picture Nominations: Exploring Data-Driven Art (2023). Visual elements of the image were determined by variables such as movie runtime and Rotten Tomatoes review scores. \n\n\n\n\n\nOverall, exploring these pieces demonstrated how generative systems have an important role to play in the future of data visualization. I resonate with Shih’s idea of creating pieces that will allow audiences to critically think and reflect on what is being is displayed. That being said, it is important that visual aesthetics of data do not overpower the messages we want to send to people, which I feel may be a notable issue to look out for. \nII. A Lesson in Data Visualization: Critiquing a Chart\nNow that we've discussed the foundations of good data visualization in class these last few weeks, I used this assignment to exercise my knowledge in critiquing a graphic in the media. I ended up coming across this article on NBC News, that discussed the opinions of Americans towards race relations at the time. Written by Carrie Dann, NBC/WSJ Poll: Americans Pessimistic on Race Relations (2017) contained two charts demonstrating poll results provided by NBC News and Wall Street Journal. While I feel it is important to keep up with topics the article discussed, I took issue with one of the charts in the article. Pictured below, the chart displays the percentage of poll respondents that perceive race relations in the U.S. as good or bad over time, from 1994-2017.\n\n\n\n\n\nLooking at this chart, there is a significant time period when race relations in the U.S. were viewed positively by Americans overall, and there are clear points where we can see a shift in public opinion. However, it's important to note that the periods of time between each data point are not equal in length, despite being displayed with equal horizontal distance between each other on the x-axis. For example, there is only a 4-month difference between September 9 to January 2010, but there is a 22-month difference from January 2010 to the time of the next set of data points, November 2011. The most notable time jump is between October 1995 to September 2005. Although this seems to be indicated by an unlabeled mark on the x-axis, the shift from a majority of \"total bad\" to \"total good\" participants may not have occurred at a constant rate as displayed on the graph. It may have been more appropriate to indicate a visual break in the main area of the graph as well. \nFurthermore, there are certain parts of the chart that could've been labeled better. For example, it may have been helpful to label the x-axis as displaying month and year - one person could mistake a date such as \"7/13\" as July 13, not July 2013. The chart also displayed the percentage for each data point of the chart. While this could be helpful in context, the size of the chart makes it difficult for viewers to easily process the numbers. The size also caused some overlap between the percentages and lines, making it hard to read.\nAn increase in chart size would not only address the issue of readability, but it could allow for the author to insert smaller captions regarding important events. For example, the article mentions the inauguration of Barack Obama and the verdict of the trial of George Zimmerman. These events, if mentioned in the chart, could give more context to various patterns in the graph. Since the news article is on the shorter side, I can understand why the author may have chosen to publish the graph the way it is. However, I think the chart can overall benefit from a larger size and more context to time differences between its data points.\nIII. Exploring Ascombe.R\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n\nIV. Changing Colors on Fall.R\n\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\ninstall.packages(\"gsubfn\", repos = \"http://cran.us.r-project.org\")\n\n\nThe downloaded binary packages are in\n    /var/folders/bh/qlhrj46x1cg_znsb_xzm3lmm0000gn/T//RtmpxEUnm7/downloaded_packages\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n\nThe downloaded binary packages are in\n    /var/folders/bh/qlhrj46x1cg_znsb_xzm3lmm0000gn/T//RtmpxEUnm7/downloaded_packages\n\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"darkolivegreen3\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes"
  },
  {
    "objectID": "posts/Assignment 2/index.html",
    "href": "posts/Assignment 2/index.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Hello! This assignment contains 3 sections:\n\nReview: Edward Tufte’s “The Future of Data Analysis\nLearning R Through Paul Murrell’s Examples\nApplying R Basics to Happy Planet Index Data\n\nI. Review: Edward Tufte’s “The Future of Data Analysis\nFor this week’s assignment, I was able to watch “The Future of Data Analysis,” the keynote speech given by Edward Tufte at Microsoft’s Machine Learning & Data Science Summit in 2016. As I desire to work in an information-design related role in the future, I believe there are a lot of great takeaways from this talk to consider in my future work.\nOne aspect that I really liked about Tufte’s discussion is that he brought in examples of the role of data visualization across various fields. For example, Tufte began with introducing a visualization of vaccination rates during the COVID-19 pandemic. However, he also noted how visualization can be influenced and utilized in other fields. He does not downplay the power of visualization in engaging others, which he says is the key to communicating strong data effectively. At the same time, Tufte also stressed that there are issues in data that must be addressed. With a background in psychology, I nodded along when the replication crisis in the field was mentioned. Tufte’s advice - prespecify important factors when dealing with a confirmatory study - falls in line with what I’ve learned so far about processes of data visualization and guidelines for my field of user research.\nI also very much enjoyed some words of wisdom that Tufte gave during this talk. At one point, Tufte mentions that it’s important to have an open mind, but not an empty head. This is similar to a piece of advice I had received - “hold strong opinions loosely.” I believe that it’s important to critically analyze the world around you and be able to form a viewpoint from the data you can collect. However, as time and other factors shift, more observations can be made that might not fall in line with the patterns that occurred earlier. Therefore, it’s important to know what constitutes good data, and how you can apply it to topics that must be addressed.\nII. Learning R Through Paul Murrell’s Examples\nThe following section is the result of observing Paul Murrell’s examples to learn R. Comments/Questions in the code have been addressed below:\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=24)  # Can you change pch? Author: changed to triangle-shaped data points\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols) for each image on 1 page\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=1) # Try different cex value? Author reduced cex  \npoints(x, y2, pch=21, bg=\"cornflowerblue\", cex=1)  # Different background color? Author changed to blue, Reduced cex\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for? This stands for orientation of axis, 1=bottom,2=left,3=top,4=right. In this example, the bottom axis being plotted.\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\", cex=0.5)\n#changed to fit graph\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins\n\nIII. Applying R Basics to Happy Planet Index Data\nIn this section, I tried using several functions to create graphs out of the Happy Planet Index Data. The following is a result of my efforts. Some things I want to note:\n\nHaving to clean the data is an important I had to address that was not taught through observing Murrell’s Graphics!\nIt was difficult to address the variable GDP Per Capita, as it contained missing values. This is something I would like to address in the future.\nMore comments in the code were made for me to use for future reference and areas to gain more knowledge in.\n\n\nlibrary(readxl)\nhpi2019 &lt;- read_excel(\"~/Downloads/happy-planet-index-2006-2020-public-data-set.xlsx\", 2)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n\n#eliminates unneccesary columns\nhpi2019 &lt;- hpi2019[-c(1:7),]\n#eliminates unneccesary row\nhpi2019 &lt;- hpi2019[-c(4)]\n#renames column headers\ncolnames(hpi2019)[1]=\"HPI_rank\"\ncolnames(hpi2019)[2]=\"Country\"\ncolnames(hpi2019)[3]=\"ISO\"\ncolnames(hpi2019)[4]=\"Continent\"\ncolnames(hpi2019)[5]=\"Pop\"\ncolnames(hpi2019)[6]=\"Life_Exp\"\ncolnames(hpi2019)[7]=\"Wellbeing\"\ncolnames(hpi2019)[8]=\"Ecological_Footprint\"\ncolnames(hpi2019)[9]=\"HPI\"\ncolnames(hpi2019)[10]=\"Biocapacity\"\ncolnames(hpi2019)[11]=\"GDP_per_capita\"\n#delete 1st row\nhpi2019 &lt;- hpi2019[-1,]\n\n\n\n#Scatterplot Comparing Life Expectancy and Wellbeing\n#Defining Variables\nx &lt;- hpi2019$Life_Exp\ny &lt;- hpi2019$Wellbeing\n#Setting graph parameters\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(50,90), ylim=c(2, 8))\npoints(x, y, pch=21)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(50, 90, 5))\naxis(2, at=seq(2,8,1))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\ntext(60, 7, \"This graph shows a trend...\", cex=0.5)#refer to coordinates you have set\nmtext(\"Life Expectancy (years)\", side=1, line=3)\n#las 0 means sideways label\nmtext(\"Wellbeing\", side=2, line=3, las=0)\nmtext(\"Comparing Life Expectancy vs. Wellbeing in Various Countries\", side=3, line=3,cex=1.25)\n\n\n\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n#Creating a Histogram of Ecological Footprints of Countries\n#changes data to a numeric format\nhpi2019$Ecological_Footprint = as.numeric(as.character(hpi2019$Ecological_Footprint))\nY &lt;- hpi2019$Ecological_Footprint\nY[Y &lt; 0|Y &gt; 20]\n\nnumeric(0)\n\nx&lt;-seq(0,20,1)\ndn &lt;- dnorm(x)\npar(mar=c(4, 4, 4, 4))\nhist(Y, breaks=seq(0, 20,2), ylim=c(0, 0.3), \n     col=\"palevioletred2\", freq=FALSE, main = \"Distribution of Ecological Footprint Among Countries\",xlab=\"Ecological Footprint\")\nlines(density(hpi2019$Ecological_Footprint),lwd =2, col = 'black')\nabline(v = mean(hpi2019$Ecological_Footprint), col=\"darkgreen\",lwd = 1.5)\nabline(v = median(hpi2019$Ecological_Footprint), col=\"gold2\",lwd = 1.5)\n\n#legend\nlegend(15, 0.22, c(\"Mean\", \"Median\",\"Density Line\"), \n       fill = c(\"darkgreen\", \"gold2\", \"black\"), \n       bty=\"n\", cex=0.75)\n\n\n\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n\n#Creating a Boxplot\n\npar(mar=c(3, 4.1, 2, 0))\n#transforming to numeric data\nhpi2019$Pop = as.numeric(as.character(hpi2019$Pop))\nhpi2019$Life_Exp = as.numeric(as.character(hpi2019$Life_Exp))\nhpi2019$Wellbeing = as.numeric(as.character(hpi2019$Wellbeing))\nhpi2019$HPI_rank = as.numeric(as.character(hpi2019$HPI_rank))\nhpi2019$Continent = as.numeric(as.character(hpi2019$Continent))\n#back to box plot\nlabel= c(\"LA\", \"NAO\",\n                      \"WE\", \"MENA\", \"ARF\", \"SA\", \"EECA\", \"EA\")\nboxplot(hpi2019$Life_Exp ~ hpi2019$Continent,col=\"gray85\" , main = \"Life Expectancy by Region\", xlab=\"Regions\", ylab=\"Life Expectancy\", names=label)\n\n\n\n# Persp Graphic Using HPI, Life Expectancy\n#need increasing order for x and y\ntranspose &lt;- t(hpi2019)\ntranspose &lt;- as.data.frame(transpose)\nreverse2019 &lt;- rev(transpose)\nreverse2019 &lt;-t(reverse2019)\nreverse2019 &lt;- as.data.frame(reverse2019)\nreverse2019$HPI = as.numeric(as.character(reverse2019$HPI))\nx &lt;- reverse2019$HPI\ny &lt;- x\nz&lt;-matrix(reverse2019$Life_Exp,length(y),length(x))\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 15,phi = 30, expand=0.5)\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n#Creating a PieChart of Percentage of Countries by Region \npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.regions &lt;- c((nrow(hpi2019[hpi2019$Continent == '1',])/nrow(hpi2019)),(nrow(hpi2019[hpi2019$Continent == '2',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '3',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '4',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '5',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '6',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '7',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '8',])/nrow(hpi2019)))\nnames(pie.regions) &lt;- c(\"Latin America\", \"N. America & Oceania\",\n                      \"Western Europe\", \"Middle East & North Africa\", \"Africa\", \"South Asia\", \"Eastern Europe & Central Asia\", \"East Asia\")\npie(pie.regions, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n#Would love to learn more on how to order from greatest to least!"
  },
  {
    "objectID": "posts/Assignment 3/index.html",
    "href": "posts/Assignment 3/index.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Hello! This assignment contains 3 sections:\n\nReview: The Parable of Google Flu: Traps in Big Data Analysis\nReview: Wickham - Data Visualization and Data Analysis (EMBL)\nModifying Anscombe.R using base and ggplot2 graphics\n\nI. Review: The Parable of Google Flu: Traps in Big Data Analysis\nBack in 2013, one may have been familiar with Google Flu Trends (GFT), a website created by Google to keep track of influenza case estimates. At the time, the platform was heralded as being an innovative use of big data. But while the GFT had good intentions in trying to predict similar trends in data with other organizations, the GFT produced various errors than expected. What happened?\nThe GFT is an example of what some people call the big data analytics pitfall - the idea that using big data can replace traditional methods of data collection and analysis. In traditional studies, factors such as construct validity and reliability are given considerable attention, as to make sure data represents concepts as intended. And these factors are sometimes overlooked when we utilize big data. For example, the GFT considered the search term “high school basketball” to be indicative of flu incidences. The authors hypothesize that the platform conflated “winter” words with flu-case indicative words (for context, influenza season and high school basketball seasons typically occur in winter. Similar cases can be caused by a concept known as overfitting, for which occurs when a model trained from a training set does not give accurate predictions for new data. This sometimes causes the inclusion of more items than intended. Overfitting can further be explained by the practice of overparameterization, when the number of parameters used exceeds the size of the training dataset. It should be noted that this is a common practice in big data analysis.\nFinally, the article points out what I find the fatal flaw of the GFT; it relies on an algorithm that is meant to drive revenue and business needs, which can differ from what the search patterns the general population utilize for the sake of simply treating the flu. This demonstrates the need for data scientists to pay closer attention to transparency, constructs represented by algorithms, and recognize big data’s weaknesses.\nII. Review: Wickham - Data Visualization and Data Analysis (EMBL)\nIn Hadley Wickham’s keynote speech at EMBL, Wickham goes over various visualization techniques, specifically within the tidyverse package for R, and advocates for how code can be quite useful for visualization. At first, he starts out with how the tidyverse packages came to be. The need to more easily modify the visual aspects of graphics helped spur the creation of ggplot, which has since evolved into ggplot2. But as Wickham noted and in my experience as well, creating the visualization is not the most difficult part of the process. Formatting the data correctly, however, can be lengthy, which inspired to creation of reshape, now in its current iteration of tidyr. The dplyr and purrr packages were further created to help aggregate data correctly. And thus, came the creation of the tidyverse.\nThrough walking through a visualization example using ggplot2, one can see how such packages were created with the grammar of graphics in mind. For example, the geom_point() function can easily control the aesthetics of different visual components. Another function helps control the scale of the data, another noted step in the GoG theory. Wickham claims that this structure, which enables easy modification to orthogonal, independent components, greatly increases users’ ability to explore data and discover greater connections.\nWickham also acknowledges the advantages coding may have in data visualization over other technologies, such as the spreadsheet interfaces of Tableau and Excel. It is said that non-coding platforms have a much nicer learning curve and can be less time-consuming than that of coding platforms. However, much of these platforms have default settings that do not communicate data as effectively as intended. I also personally find it more difficult to replicate the formatting of data presentation (i.e. formulas) for other data sets when using such tools. Meanwhile, coding allows for easy reproduction, a wider range of customization, and a greater ability for users to explore the insights data can offer.\n\nIII. Modifying Anscombe.R using base and ggplot2 graphics\nAs shown in Assignment 1, Ascombe (1973) displays different graphs that are shown to have the same r-coefficient via linear regression analysis. The problem that this data showcases is that simply knowing the r-value is NOT enough to have a complete picture of the data. Graph 1 (Top Row, Left) does not have any problems - the data point distribution strongly suggests a positive, linear relationship. In contrast, Graph 2 (Top Row, Right) suggests a non-linear fit model. The bottom row graphs also hint at the effect of an outlier on its coefficient correlation.\nThe graphs from Ascombe can be seen below. Note that these graphs are modified from the originals, as I played around with the code to experiment with changing colors, line types, plotting characters and fonts without packages:\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0), family=\"Times New Roman\")\n#family in par -&gt; changes fonts in graph\n#for Windows users, you can also use windowsFont(), unfortunately, I have a Mac...\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"midnightblue\",type=\"p\", pch = 23, bg = \"lightseagreen\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"firebrick\", lwd=2, lty=2)\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n#plotting character can be changed using pch\n#type indicates whether points (p), lines(l), or both(b) should be plotted\n#lty changes the type of line (dotted, semi dotted)\n\nNext, I wanted to see how I could change the plots using ggplot2 (the tidyverse package). These are the results of my experimentation below:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\n\n\n\n## Simple version\nggplot(anscombe, aes(x=x1, y=y1))+geom_point()\n\n\n\n## geom_point changes graph characteristics\n## can change size, shape, color\nggplot(anscombe, aes(x=x1, y=y1)) + geom_point(size=2, shape=5)\n\n\n\n## if you wnat to just change size of POINTS, use (aes(size=1)) within geom_point\n## if you ever wanted to label points, use geom_text(label=rownames(anscombe))\n\n##plot regression line using geom_smooth\n#specify for linear with method=lm, delete range with se=FALSE\n\nggplot(anscombe, aes(x=x1, y=y1)) + geom_point(size=2, shape=17,color=\"lightseagreen\") + geom_smooth(method=lm, se=FALSE,color=\"firebrick3\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n#you can change the line type and color within geom_smooth as well\nggplot(anscombe, aes(x=x2, y=y2)) + geom_point(size=2, shape=17,color=\"mediumpurple3\") + geom_smooth(method=lm, se=FALSE,color=\"darkorange4\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(anscombe, aes(x=x3, y=y3)) + geom_point(size=2, shape=17,color=\"palegreen3\") + geom_smooth(method=lm, se=FALSE,color=\"deeppink4\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(anscombe, aes(x=x4, y=y4)) + geom_point(size=2, shape=17,color=\"violetred1\") + geom_smooth(method=lm, se=FALSE,color=\"hotpink4\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n#doing with this with other graphs\n\nIV. Running the Pre-Hackathon Data\n\n## Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\nView(owidall)\n#Deselect cases/rows with OWID\n#grepl() searches for matches in characters/sequences of characters in string\nowidall = owidall[!grepl(\"^OWID\",owidall$iso_code), ]\nView(owidall)\n#Subset by continent: Europe\nowideu = subset(owidall, continent==\"Europe\")\nView(owideu)\n\n# Europe data\ny = owideu$new_deaths\n#as.Date converts character objects to date objects\nx = as.Date(owideu$date)\n#xaxt gives the style, 'n' means no scale\nplot(x,y, pch=20, col=\"#E7298A\", cex = .5, xaxt='n', xlab = \"Date\", ylab = \"COVID Deaths in Europe (Daily)\")\n#note for format - %Y-%m gives the dates in year/month format\n#tick marks - in this example we do not display\naxis(1, x, format(x, \"%Y-%m\"), cex.axis = .7, las = 3 , gap.axis =1.5, tick = FALSE)\nidentify(x,y,owideu$location, ps=8, atpen=TRUE) # Manually identify cases by mouse click\n\n\n\n\ninteger(0)"
  },
  {
    "objectID": "posts/Assignment 5/index.html",
    "href": "posts/Assignment 5/index.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Plotting Without Packages\n\ndata(Seatbelts)\nseatbelt &lt;- data.frame (Year=floor(time(Seatbelts)) ,\nMonth=factor(cycle(Seatbelts) ,\nlabels=month.abb) , Seatbelts)\n# floor() returns largest integer smaller or equal\n# time looks for execution time? establishes time series\n# cycle just repeats it?\n# month.abb means three letter abbreviation for English months\nstr(seatbelt)\n\n'data.frame':   192 obs. of  10 variables:\n $ Year         : Time-Series  from 1969 to 1985: 1969 1969 1969 1969 1969 ...\n $ Month        : Factor w/ 12 levels \"Jan\",\"Feb\",\"Mar\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ DriversKilled: num  107 97 102 87 119 106 110 106 107 134 ...\n $ drivers      : num  1687 1508 1507 1385 1632 ...\n $ front        : num  867 825 806 814 991 ...\n $ rear         : num  269 265 319 407 454 427 522 536 405 437 ...\n $ kms          : num  9059 7685 9963 10955 11823 ...\n $ PetrolPrice  : num  0.103 0.102 0.102 0.101 0.101 ...\n $ VanKilled    : num  12 6 12 8 10 13 11 6 10 16 ...\n $ law          : num  0 0 0 0 0 0 0 0 0 0 ...\n\n#factors -&gt; categorical variables - fixed, known set - think month - correlated number\nhead(seatbelt)\n\n  Year Month DriversKilled drivers front rear   kms PetrolPrice VanKilled law\n1 1969   Jan           107    1687   867  269  9059   0.1029718        12   0\n2 1969   Feb            97    1508   825  265  7685   0.1023630         6   0\n3 1969   Mar           102    1507   806  319  9963   0.1020625        12   0\n4 1969   Apr            87    1385   814  407 10955   0.1008733         8   0\n5 1969   May           119    1632   991  454 11823   0.1010197        10   0\n6 1969   Jun           106    1511   945  427 12391   0.1005812        13   0\n\n#Create a histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\nY &lt;- seatbelt$PetrolPrice\ndn &lt;- dnorm(x)\nhist(Y) #simple\n\n\n\nhist(Y,col=\"steelblue\")\nlines(density(seatbelt$PetrolPrice),lwd =2, col = 'black')\n\n\n\n#Create a Scatterplot\nx &lt;-seatbelt$front\ny &lt;-seatbelt$rear\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(400,1400), ylim=c(200, 700))\npoints(x, y, pch=5, cex=0.5)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(400, 1400, 300))\naxis(2, at=seq(200,700,100))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\nmtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\nmtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\nmtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\n\n\n\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n#HorizontalBarChart\nseatbelt$decade &lt;- 10 * (seatbelt$Year %/% 10)\nfatalitiesbydecade &lt;- table(seatbelt$decade)\nbarplot(fatalitiesbydecade, \n        main = \"Deaths by Decade\",\n        xlab = \"Decade\",\n        ylab = \"Number of Deaths\",\n        col = \"darkolivegreen4\",\n        horiz=TRUE)\n\n\n\n#VerticalBarChart\n\ndeaths_by_year &lt;- tapply(seatbelt$DriversKilled, seatbelt$Year, sum)\n\nbarplot(deaths_by_year,\n        main = \"Total Deaths by Year\",\n        xlab = \"Year\",\n        ylab = \"Total Deaths\",\n        col = \"darkolivegreen4\",\n        names.arg = unique(seatbelt$Year),\n        ylim = c(0, max(deaths_by_year) + 20)) \n\n\n\n#BoxPlot\nboxplot(seatbelt$DriversKilled,\n        main = \"Boxplot of Drivers Killed\",\n        ylab = \"Deaths\",\n        col = \"darkorange3\",\n        border = \"black\")\n\n\n\n#PieChart - mtcars \n#note to self - take note of which datsets are appropriate for different forms of graphs in the future\n\ndata(mtcars)\n\n# Create a pie chart showing the distribution of cylinder counts\ndata(mtcars)\nView(mtcars)\ncylinder_counts &lt;- table(mtcars$cyl)\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie(cylinder_counts, \n    labels = names(cylinder_counts), \n    main = \"How Many Cylinders?\", col = gray(seq(0.3,1.0,length=6)))\n\n\n\n\nPlotting With GGPlot\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(Seatbelts)\nseatbelt &lt;- data.frame (Year=floor(time(Seatbelts)) ,\nMonth=factor(cycle(Seatbelts) ,\nlabels=month.abb) , Seatbelts)\n# floor() returns largest integer smaller or equal\n# time looks for execution time? establishes time series\n# cycle just repeats it?\n# month.abb means three letter abbreviation for English months\nView(seatbelt)\nstr(seatbelt)\n\n'data.frame':   192 obs. of  10 variables:\n $ Year         : Time-Series  from 1969 to 1985: 1969 1969 1969 1969 1969 ...\n $ Month        : Factor w/ 12 levels \"Jan\",\"Feb\",\"Mar\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ DriversKilled: num  107 97 102 87 119 106 110 106 107 134 ...\n $ drivers      : num  1687 1508 1507 1385 1632 ...\n $ front        : num  867 825 806 814 991 ...\n $ rear         : num  269 265 319 407 454 427 522 536 405 437 ...\n $ kms          : num  9059 7685 9963 10955 11823 ...\n $ PetrolPrice  : num  0.103 0.102 0.102 0.101 0.101 ...\n $ VanKilled    : num  12 6 12 8 10 13 11 6 10 16 ...\n $ law          : num  0 0 0 0 0 0 0 0 0 0 ...\n\n#factors -&gt; categorical variables - fixed, known set - think month - correlated number\nhead(seatbelt)\n\n  Year Month DriversKilled drivers front rear   kms PetrolPrice VanKilled law\n1 1969   Jan           107    1687   867  269  9059   0.1029718        12   0\n2 1969   Feb            97    1508   825  265  7685   0.1023630         6   0\n3 1969   Mar           102    1507   806  319  9963   0.1020625        12   0\n4 1969   Apr            87    1385   814  407 10955   0.1008733         8   0\n5 1969   May           119    1632   991  454 11823   0.1010197        10   0\n6 1969   Jun           106    1511   945  427 12391   0.1005812        13   0\n\n#Create a histogram - original\n# Y &lt;- seatbelt$PetrolPrice\n# dn &lt;- dnorm(x)\n# hist(Y) #simple\n# hist(Y,col=\"steelblue\")\n# lines(density(seatbelt$PetrolPrice),lwd =2, col = 'black')\n\nY &lt;- seatbelt$PetrolPrice\nggplot(data = data.frame(Y = Y), aes(x = Y)) +\n  geom_histogram(color = 'black', fill = 'steelblue', bins = 30) +\n  geom_density(color = 'black', linewidth = 1) +\n  labs(title = 'Histogram of Petrol Price',\n       x = 'Petrol Price',\n       y = 'Frequency') +\n  theme_minimal()\n\n\n\n#Create a Scatterplot (basics notes)\n# x &lt;-seatbelt$front\n#y &lt;-seatbelt$rear\n#par(las=1, mar=c(4, 4, 5, 4))\n#plot.new()\n#plot.window(xlim=c(400,1400), ylim=c(200, 700))\n#points(x, y, pch=5, cex=0.5)\n#par(col=\"black\", fg=\"black\", col.axis=\"black\")\n#axis(1, at=seq(400, 1400, 300))\n#axis(2, at=seq(200,700,100))\n##plotting a box on just 2 axes\n#box(bty=\"L\")\n#mtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\n#mtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\n#mtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\n#par(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n#Scatterplot ggplot2\nx &lt;-seatbelt$front\ny &lt;-seatbelt$rear\nggplot(data = data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point(shape = 24, size = 3, fill = \"mediumpurple3\") +\n  xlim(400, 1400) +\n  ylim(200, 700) +\n  labs(title = \"Comparing Seatbelt Related Incidences\",\n       x = \"Front Related Incidences\",\n       y = \"Rear Related Incidences\") +\n  theme_minimal() \n\n\n\n#HorizontalBarChart\nseatbelt$decade &lt;- 10 * (seatbelt$Year %/% 10)\nfatalitiesbydecade &lt;- table(seatbelt$decade)\nggplot(data = data.frame(decade = as.factor(names(fatalitiesbydecade)),\n                         deaths = as.numeric(fatalitiesbydecade)), \n       aes(x = decade, y = deaths, fill = decade)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Deaths by Decade\",\n       x = \"Decade\",\n       y = \"Number of Deaths\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"darkolivegreen4\", \"darkolivegreen2\", \"darkolivegreen4\")) +\n  coord_flip()\n\n\n\n#VerticalBarChart\n\ndeaths_by_year &lt;- tapply(seatbelt$DriversKilled, seatbelt$Year, sum)\ndeaths_by_year_df &lt;- data.frame(Year = as.numeric(names(deaths_by_year)),\n                                TotalDeaths = as.numeric(deaths_by_year))\n\nggplot(data = deaths_by_year_df, aes(x = factor(Year), y = TotalDeaths, fill = factor(Year))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Total Deaths by Year\",\n       x = \"Year\",\n       y = \"Total Deaths\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"darkolivegreen4\", \"darkolivegreen4\", \"darkolivegreen4\", \"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\",\"darkolivegreen4\")) +\n  ylim(0, max(deaths_by_year) + 20)\n\n\n\n#BoxPlot boxplot(seatbelt$DriversKilled,\n# main = \"Boxplot of Drivers Killed\",\n# ylab = \"Deaths\",\n#col = \"darkorange3\",\n# border = \"black\")\n\nggplot(data = data.frame(DriversKilled = seatbelt$DriversKilled), \n       aes(y = DriversKilled)) +\n  geom_boxplot(color = \"black\", fill = \"darkorange3\") +\n  labs(title = \"Boxplot of Drivers Killed\",\n       ylab = \"Deaths\") +\n  theme_minimal()\n\n\n\n#PieChart - mtcars \n#note to self - take note of which datsets are appropriate for different forms of graphs in the future\n\ndata(mtcars)\n\n# Create a pie chart showing the distribution of cylinder counts\ndata(mtcars)\nView(mtcars)\ncylinder_counts &lt;- table(mtcars$cyl)\n\n#Data Frame\npie_data &lt;- data.frame(\n  cylinder = factor(names(cylinder_counts), levels = names(cylinder_counts)),\n  count = as.numeric(cylinder_counts)\n)\n\n# Calculate percentages for the labels\npie_data$percent &lt;- pie_data$count / sum(pie_data$count) * 100\npie_data$label &lt;- paste0(pie_data$cylinder, \"\\n\", round(pie_data$percent, 1), \"%\")\n\n#actual plot\nggplot(data = pie_data, aes(x = \"\", y = percent, fill = cylinder)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\") +\n  theme_void() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"How Many Cylinders?\") +\n  geom_text(aes(label = label), position = position_stack(vjust = 0.5))\n\n\n\n\nExporting Charts\n\ndata(Seatbelts)\nseatbelt &lt;- data.frame (Year=floor(time(Seatbelts)) ,\nMonth=factor(cycle(Seatbelts) ,\nlabels=month.abb) , Seatbelts)\nx &lt;-seatbelt$front\ny &lt;-seatbelt$rear\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(400,1400), ylim=c(200, 700))\npoints(x, y, pch=5, cex=0.5)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(400, 1400, 300))\naxis(2, at=seq(200,700,100))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\nmtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\nmtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\nmtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\n\n\n\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n#Examples\n#pdf(“rplot.pdf”): pdf file\n#png(“rplot.png”): png file\n#jpeg(“rplot.jpg”): jpeg file\n#postscript(“rplot.ps”): postscript file\n#bmp(“rplot.bmp”): bmp file\n#win.metafile(“rplot.wmf”): windows metafile\n\n#png\npng(\"scatterplot.png\")\n\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(400,1400), ylim=c(200, 700))\npoints(x, y, pch=5, cex=0.5)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(400, 1400, 300))\naxis(2, at=seq(200,700,100))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\nmtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\nmtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\nmtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\ndev.off()\n\nquartz_off_screen \n                2 \n\n#jpg\npng(\"scatterplot.jpg\")\n\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(400,1400), ylim=c(200, 700))\npoints(x, y, pch=5, cex=0.5)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(400, 1400, 300))\naxis(2, at=seq(200,700,100))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\nmtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\nmtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\nmtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\ndev.off()\n\nquartz_off_screen \n                2 \n\n#svg\npng(\"scatterplot.svg\")\n\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(400,1400), ylim=c(200, 700))\npoints(x, y, pch=5, cex=0.5)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(400, 1400, 300))\naxis(2, at=seq(200,700,100))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\nmtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\nmtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\nmtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\ndev.off()\n\nquartz_off_screen \n                2 \n\n#tiff\npng(\"scatterplot.tiff\")\n\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(400,1400), ylim=c(200, 700))\npoints(x, y, pch=5, cex=0.5)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(400, 1400, 300))\naxis(2, at=seq(200,700,100))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\nmtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\nmtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\nmtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\ndev.off()\n\nquartz_off_screen \n                2 \n\n#bmg\npng(\"scatterplot.bmg\")\n\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(400,1400), ylim=c(200, 700))\npoints(x, y, pch=5, cex=0.5)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(400, 1400, 300))\naxis(2, at=seq(200,700,100))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\nmtext(\"Front Related Incidences\", side=1, line=3, cex=0.8)\n#las 0 means sideways label\nmtext(\"Rear Related Incidences\", side=2, line=3, las=0, cex=0.8)\nmtext(\"Comparing Seatbelt Related Incidences\", side=3, line=3,cex=1)\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\ndev.off()\n\nquartz_off_screen \n                2 \n\n#notes - PNG has the highest quality, whereas jpg and tiff have lower qualities. We cannot access the svg and bmg as presented with full graphics."
  },
  {
    "objectID": "posts/Assignment 4/index.html",
    "href": "posts/Assignment 4/index.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Final Output Hackathon Assignment (some practice for code is below as well)\n\n# Load necessary libraries if not already loaded\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n#load dataset\nroaddeaths &lt;- read.csv(\"~/Downloads/Road traffic accidents.csv\")\n# Filter the dataset for the year 2021\nyear_2019_data &lt;- subset(roaddeaths, Year == 2019)\n\n# Calculate the total deaths for all ages and both sexes in 2019\ntotal_deaths_2019 &lt;- year_2019_data %&gt;%\n  filter(Sex == \"All\" & Age.Group == \"[All]\")\n\n# Specify the number of top countries you want (e.g., top 5)\nT &lt;- 5\n# Get the top T countries with the highest road accidents in 2019\ntop_5_countries &lt;- total_deaths_2019 %&gt;%\n  top_n(T, Number)\n#Some calculations for ploting\ntop_5_countries$wc &lt;- cumsum(top_5_countries$Death.rate.per.100.000.population)\ntop_5_countries$sp &lt;-top_5_countries$wc - top_5_countries$Death.rate.per.100.000.population\ntop_5_countries$mp &lt;- with(top_5_countries,sp +(wc-sp)/2)\n# plotting\n\ncustom_colors &lt;- c(\"Brazil\" = \"firebrick\", \"Mexico\" = \"navy\", \n                   \"Russian Federation\" = \"seagreen\", \"Thailand\" = \"chocolate\", \n                   \"United States of America\" = \"orchid\")\nggplot(top_5_countries, aes(ymin= 0)) +\n  geom_rect((aes(xmin = sp, xmax = wc, ymax = Number, fill = Country))) +\n  geom_text(aes(x = mp, y = Number * 0.5, label = c(\"Brazil\", \"Mexico\", \n                                                    \"Russia\", \"Thailand\", \"USA\"))) + \n  theme_bw() + \n  theme(legend.position = \"none\") + \n  labs(\n    title = \"Road Deaths and Death Rates for Countries with the Highest Mortality\",\n    x = \"Death Rate (per 100,000 population)\",\n    y = \"Number of Road Accident Deaths\",\n  ) +\n  scale_fill_manual(values = custom_colors)\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#load dataset\nroaddeaths &lt;- read.csv(\"~/Downloads/Road traffic accidents.csv\")\n# Filter the dataset for the year 2019 $ 2020\nyear_2019_2020_data &lt;- subset(roaddeaths, Year == 2019 | Year == 2020)\n\n# Calculate the total deaths for all ages and both sexes in 2019 $ 2020\nroad_deaths_2019_2020 &lt;- year_2019_2020_data %&gt;%\n  filter(Sex == \"All\" & Age.Group == \"[All]\")\n\n# Specify the number of top countries\nT &lt;- 4\n# Get the top 4 countries with the highest road accident deaths\ntop_countries&lt;- road_deaths_2019_2020 %&gt;%\n  top_n(T,Number )\n\n# Define custom fill colors\ncustom_colors &lt;- c(\"2019\" = \"darkslategrey\", \"2020\" = \"orchid\")\n# Plot\nggplot(top_countries, aes(x = Country, y = Number, fill = as.factor(Year))) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  facet_grid(~ Country, scales = \"free_y\") +\n  theme(axis.text.x = element_blank()) +\n  labs(title = \"Road Accident Deaths\", x = NULL, y = \"Number of Deaths\") +\n  scale_fill_manual(values = custom_colors)\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#load dataset\nroaddeaths &lt;- read.csv(\"~/Downloads/Road traffic accidents.csv\")\n# Filter the dataset for the year 2019\nyear_2019_data &lt;- subset(roaddeaths, Year == 2019)\n# Filter the dataset for USA\nusa_2019 &lt;- subset(year_2019_data, Country == \"United States of America\" \n                   & Age.Group != \"[All]\" & Age.Group != \"[Unknown]\")\n\n# Reorder the levels of Age.Group\nusa_2019$Age.Group &lt;- factor(usa_2019$Age.Group, \n                             levels = c(\"[0]\", \"[1-4]\", \"[5-9]\", \"[10-14]\", \"[15-19]\",\n                                        \"[20-24]\", \"[25-29]\", \"[30-34]\", \"[35-39]\",\n                                        \"[40-44]\", \"[45-49]\", \"[50-54]\", \"[55-59]\", \n                                        \"[60-64]\", \"[65-69]\", \"[70-74]\", \"[75-79]\", \n                                        \"[80-84]\", \"[85+]\"))\n\n# Create the bar plot\n\nggplot(usa_2019, aes(x = Number)) +\n  geom_bar(aes(y = Age.Group), position = \"dodge\", stat = \"identity\", fill = \"darkslategrey\") +\n  labs(\n    title = \"Road Accident Deaths in the USA by Age Group (2019)\",\n    x = \"Number of Road Traffic Accidents\",\n    y = \"Age Group\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n# Load the necessary libraries\nlibrary(ggplot2)\n#load dataset\nroaddeaths &lt;- read.csv(\"~/Downloads/Road traffic accidents.csv\")\n# Filter the dataset for the year 2021\nyear_2019_data &lt;- subset(roaddeaths, Year == 2019)\n# Create a clustered bar chart\n\nggplot(year_2019_data, aes(x = Region)) +\n  geom_col(aes(y = Number, fill = Sex), position = \"dodge\", stat = \"identity\") +\n  scale_fill_manual(values = c(\"Male\" = \"orchid\", \"Female\" = \"chocolate\", \"All\" = \"darkslategrey\")) +\n  labs(\n    title = \"Road Accident Deaths by Gender and Region (2019)\",\n    x = NULL,\n    y = \"Number of Road Traffic Accidents\",\n    fill = \"Gender\"\n  ) +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning in geom_col(aes(y = Number, fill = Sex), position = \"dodge\", stat =\n\"identity\"): Ignoring unknown parameters: `stat`\n\n\nWarning: Removed 27 rows containing missing values (`geom_col()`).\n\n\n\n\n\nPractice 1\n\nrt &lt;- read.csv(\"~/Downloads/Road traffic accidents.csv\")\n\n##practice variable width\n\n#downloading libraries\nlibrary(ggplot2)\nlibrary(hrbrthemes)\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n\n\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n\n\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n#make data\n#each include column\ndata &lt;- data.frame(\n  group=c(\"A\" ,\"B\" ,\"C\" ,\"D\" ) ,\n  value=c(33,62,56,67) ,\n  number_of_obs=c(100,500,459,342)\n)\nView(data)\n\n\n# Calculate the future positions on the x axis of each bar (left border, central position, right border)\n#cumsum() adds the values of each column to that point, including the 30 added to right\n#added 30 to each\ndata$right &lt;- cumsum(data$number_of_obs) + 30*c(0:(nrow(data)-1))\n#creates left border?\ndata$left &lt;- data$right - data$number_of_ob\nView(data)\n\n# Plot\n#xmin,xmax,ymin,ymax specify annotations of rectangles\n#xend,yend geometric curves\n#aes in gg plot modifies overall plot\n#colour = group -&gt; authomatic color?\nggplot(data, aes(ymin = 0)) + \n    geom_rect(aes(xmin = left, xmax = right, ymax = value, colour = group, fill = group)) +\n    xlab(\"number of obs\") + \n    ylab(\"value\") +\n    theme_ipsum() +\n    theme(legend.position=\"none\") \n\n\n\n\nChart 1\n\nrt &lt;- read.csv(\"~/Downloads/Road traffic accidents.csv\")\nchart1 &lt;- data.frame(\n  region=c(\"AF\" ,\"AS\" ,\"CSA\" ,\"EU\", \"NAC\", \"OA\")\n)\nView(chart1)\n\nPractice 2\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()          masks stats::filter()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ dplyr::lag()             masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(kableExtra)\ndt &lt;- mtcars[1:5, 1:6]\nView(dt)\n\n#displays data set in HTML format\nkbl(dt)\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n\n\n\n\n\n\n#styling applies bootstrap\ndt %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n\n\n\n\n\n\n#other formats available - see https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n#can add caption in kbl\n\ndt %&gt;%\n  kbl(caption = \"Displaying Car Data\") %&gt;%\n  kable_styling()\n\n\nDisplaying Car Data\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n\n\n\n\n\n\nmpg_list &lt;- split(mtcars$mpg, mtcars$cyl)\ndisp_list &lt;- split(mtcars$disp, mtcars$cyl)\ninline_plot &lt;- data.frame(cyl = c(4, 6, 8), mpg_box = \"\", mpg_hist = \"\",\n                          mpg_line1 = \"\", mpg_line2 = \"\",\n                          mpg_points1 = \"\", mpg_points2 = \"\", mpg_poly = \"\")\ninline_plot %&gt;%\n  kbl(booktabs = TRUE) %&gt;%\n  kable_paper(full_width = FALSE) %&gt;%\n  column_spec(2, image = spec_boxplot(mpg_list)) %&gt;%\n  column_spec(3, image = spec_hist(mpg_list)) %&gt;%\n  column_spec(4, image = spec_plot(mpg_list, same_lim = TRUE)) %&gt;%\n  column_spec(5, image = spec_plot(mpg_list, same_lim = FALSE)) %&gt;%\n  column_spec(6, image = spec_plot(mpg_list, type = \"p\")) %&gt;%\n  column_spec(7, image = spec_plot(mpg_list, disp_list, type = \"p\")) %&gt;%\n  column_spec(8, image = spec_plot(mpg_list, polymin = 5))\n\n\n\n\ncyl\nmpg_box\nmpg_hist\nmpg_line1\nmpg_line2\nmpg_points1\nmpg_points2\nmpg_poly\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n\n\n\n\n\nView(mtcars)"
  },
  {
    "objectID": "posts/Assignment 7/index.html",
    "href": "posts/Assignment 7/index.html",
    "title": "Assignment 7",
    "section": "",
    "text": "library(tidyverse) library(dplyr) library(readxl) #Load dataset WDI &lt;- read_excel(“Downloads/WDI.xlsx”) View(WDI)\n#Cleaning Data WDI &lt;- WDI[ , -4]"
  },
  {
    "objectID": "posts/Assignment 9/index.html",
    "href": "posts/Assignment 9/index.html",
    "title": "Assignment 9",
    "section": "",
    "text": "library(quantmod) \n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n#download sample data\ngetSymbols(\"AAPL\",\n           from = \"2020/12/31\",\n           to = \"2022/12/31\",\n           periodicity = \"daily\")\n\n[1] \"AAPL\"\n\n## 'getSymbols' currently uses auto.assign=TRUE by default, but will\n## use auto.assign=FALSE in 0.5-0. You will still be able to use\n## 'loadSymbols' to automatically load data. getOption(\"getSymbols.env\")\n## and getOption(\"getSymbols.auto.assign\") will still be checked for\n## alternate defaults.\n## \n## [1] \"AAPL\"\n\n#view data\nhead(AAPL)\n\n           AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted\n2020-12-31    134.08    134.74   131.72     132.69    99116600      130.5590\n2021-01-04    133.52    133.61   126.76     129.41   143301900      127.3317\n2021-01-05    128.89    131.74   128.43     131.01    97664900      128.9060\n2021-01-06    127.72    131.05   126.38     126.60   155088000      124.5668\n2021-01-07    128.36    131.63   127.86     130.92   109578200      128.8174\n2021-01-08    132.43    132.63   130.23     132.05   105158200      129.9293\n\n#find class\nclass(AAPL)\n\n[1] \"xts\" \"zoo\"\n\n# The class is xts and zoo\n\nPlotting TSStudio\n\n# Plotting time series data using TSstudio\n# install.packages(c(\"quantmod\", \"tidyverse\",\"TSstudio\"))\n# lapply(c(\"quantmod\", \"tidyverse\",\"TSstudio\"), require, character.only = TRUE)\n\n\nlapply(c(\"quantmod\", \"tidyverse\",\"TSstudio\"), require, character.only = TRUE)\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::first()  masks xts::first()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::last()   masks xts::last()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: TSstudio\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\nlibrary(TSstudio)\nquantmod::getSymbols(\"AAPL\", src=\"yahoo\")\n\n[1] \"AAPL\"\n\nclass(AAPL)\n\n[1] \"xts\" \"zoo\"\n\nts_plot(AAPL$AAPL.Adjusted, \n        title = \"Apple Stock prices\",\n        Ytitle = \"\")\n\n\n\n\nclass(AAPL) # What class is this object? - It is xts and zoo.\n\n[1] \"xts\" \"zoo\"\n\n# Some sample dataset from TSstudio\nts_seasonal(USgas) # month-year matrix data\n\n\n\n\n#omitted type = \"\" due to error, can insert different kinds such as additive, multiplicative, etc. \n\n\n# What class is USgas?\nclass(USgas)\n\n[1] \"ts\"\n\n# USgas is ts\n\n\n# Sample charts\nts_heatmap(USgas)\n\n\n\n\n#Trend - appears to peak during January and be the least during June/May. The data indicates a seasonal trend.\n#Stationarity - appears to return - surround the average. Below average during mid-year months, above average during January and adjacent months.\n#PDQ\n\n\nts_cor(USgas) # ACF and PACF\n\n\n\n\n#Trend - appears to peak during January and be the least during June/May. The data indicates a seasonal trend.\n#Stationarity - appears to return - surround the average. Below average during mid-year months, above average during January and adjacent months.\n#PDQ\n\nts_lags(USgas, margin = .01)\n\n\n\n\n#Trend - postitive trend correlation in lags. some are more clear to see, others less clear - scattered data points and likely not significant\n#Stationarity - clear increase on some lags, other lags - datapoint is scattered enough it may center around a mean\n#PDQ\n\n#to find PDQ use auto.arima - forecast\n\nlibrary(forecast)\nbestfit &lt;- auto.arima(USgas)\n\n# This is the pdq:\nsummary(bestfit)\n\nSeries: USgas \nARIMA(2,1,1)(2,1,1)[12] \n\nCoefficients:\n         ar1      ar2      ma1     sar1    sar2     sma1\n      0.4092  -0.0241  -0.9059  -0.0029  -0.270  -0.7209\ns.e.  0.0525   0.0929   0.0509   0.0682   0.111   0.0664\n\nsigma^2 = 10588:  log likelihood = -1366.5\nAIC=2747   AICc=2747.52   BIC=2770.92\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 6.207831 98.70513 74.24104 0.1251525 3.528373 0.6386521\n                     ACF1\nTraining set -0.005118595\n\nusgas=data.frame(USgas)\n\nPlotting DYGraphs\n\n# Plotting time series data using dygraph\n# install.packages(c(\"quantmod\", \"tidyverse\",\"dygraphs\"))\n# lapply(c(\"quantmod\", \"tidyverse\",\"dygraphs\"), require, character.only = TRUE)\n\nlapply(c(\"quantmod\", \"tidyverse\",\"dygraphs\"), require, character.only = TRUE)\n\nLoading required package: dygraphs\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\nlibrary(dygraphs)\npar(family=\"Palatino\")\nquantmod::getSymbols(\"GOOGL\", src=\"yahoo\")\n\n[1] \"GOOGL\"\n\n#changed from Twitter due to closed data :(\n\nclass(GOOGL)\n\n[1] \"xts\" \"zoo\"\n\n# is xts and zoo\nm = tail(GOOGL, n=30)\nm =m[,1:(ncol(m)-2)] # drops last two columns \nnames(m)&lt;-c('Open', 'High', 'Low', 'Close') # renames columns for plotting\ngetwd()\n\n[1] \"/Users/jannellenavales/Downloads/EPPS6356/jannellenavalesblog/posts/Assignment 9\"\n\n#allows you to see working directory - where Assignment 9 should go\n#in this case, it's in assignment 9 \n\ndygraph(m, main = \"Google Stock Prices (Candlestick Chart)\") |&gt;\n  dyCandlestickGroup(c('Open', 'High', 'Low', 'Close')) |&gt;\n  dyCandlestick() |&gt;\n  dyLegend(show = \"always\", hideOnMouseOut = TRUE) |&gt;\n  dyCSS(\"dygraph.css\")\n\n\n\n\n#Trend = slight increase through most of October, before a drop. Seems to be increasing again. Not sure if this is part of a larger cyle of increasing/decreasing over time?\n#Stationarity = the data as currently presetented? None. But if part of a larger increase/decrease cycle, maybe."
  },
  {
    "objectID": "posts/Assignment 8/index.html",
    "href": "posts/Assignment 8/index.html",
    "title": "Assignment 8",
    "section": "",
    "text": "This week, I mostly brainstormed ideas for dashboards and looked up examples of R Markdown files. Unfortunately, R studio does not support a dashboard yet."
  }
]