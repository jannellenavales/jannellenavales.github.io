[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Assignment Blog",
    "section": "",
    "text": "Hello! This is my 1st Assignment.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nplot(iris)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello there! I am currently a graduate student in The University of Texas at Dallas’ Applied Cognition & Neuroscience Program, specializing in Human-Computer Interaction. With a background in psychology, I have always been interested in using my knowledge to support those in need. When I first learned about user experience (UX), I immediately fell in love with the field’s multidisciplinary nature and evidence-based process.\nI am passionate about creating digital services that are valuable and accessible to diverse user bases. I am a continuous learner and am excited to keep developing my skills through new opportunities in the future! Feel free to reach out and view my resume and/or portfolio.\nWhen I’m not working or studying, you can find me experimenting with songs on my keyboard, volunteering with friends, or exploring new worlds through literature."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Blog!",
    "section": "",
    "text": "Assignment 2\n\n\n\n\n\n\n\ndata analysis, r, cleaning data, scatterplot, barchart, histogram, boxplot, perspective chart, pie chart\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 1\n\n\n\n\n\n\n\ndata visualization\n\n\ngenerative art\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Assignment 1/index.html",
    "href": "posts/Assignment 1/index.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Hello! This assignment contains 4 sections:\n\nExamples of Generative Art\nA Lesson in Data Visualization: Critiquing a Chart\nExploring Ascombe.R\nChanging Colors on Fall.R\n\nI. Examples of Generative Art\nWhat is generative art? \nWhen I first think of this term, I immediately think of the use of artificial intelligence (AI) to create different pieces of art, drawing from other examples from the web or fed into some algorithm. Indeed, generative art often refers to pieces created by an algorithm, but to any autonomous system from various fields, such as statistics or life and physical sciences. Such systems operate as instructions, placing limits on how a work may turn out. However, different outcomes can be generated based on the amount of chance its creator introduces in applying the system. \nOne of the earliest examples of generative art includes George Nees’ Schotter (1968), as seen below. Nees used the ALGOL programming language and introduced random variables to create Schotter. While the top of the artwork begins as standard grid, the comprising squares begin to rotate and move, creating a gradient from order to chaos.  \n\n\n\n\n\n \nNowadays, generative art can be created using a greater variety of programming languages than was offered in 1968. For example, Javascript is one of the most popular languages utilized by artists today. Last spring, I took a web design class and identified several projects that could be classified as generative art. One piece in particular that stood out to me was DT Soulmate Matcher (1968), a piece created by then-MIT graduate student Munus Shih. The platform can be accessed here. A screenshot of the homepage can be found below. \n\n\n\n\n\nDT Soulmate Matcher was partly inspired by Shih’s experiences with classmates in his MFA program, who often wished they could meet more like-minded peers. Furthermore, Shih wanted to investigate the exploratory nature of data visualization and how it could be used to connect audiences. Collecting information from his peers, Shih created a system for how different traits would be visualized in the piece (I.e. patterns, distance between individual profiles) using the library p5.js. He also created an algorithm that allowed for students to be assigned a “soulmate,” or peer that they had most in common with. This system was coded in a way that it could accommodate for future student data, thus altering the initial displays and matches of the page.  \nAnother example of generative art using p5.js includes the following image, created by Ike Stevens. While it may seem the piece might initially appear random at first, it is actually created using Oscar Best Picture Nomination data. The entire process can be further explained in this Medium article, Visualized Best Picture Nominations: Exploring Data-Driven Art (2023). Visual elements of the image were determined by variables such as movie runtime and Rotten Tomatoes review scores. \n\n\n\n\n\nOverall, exploring these pieces demonstrated how generative systems have an important role to play in the future of data visualization. I resonate with Shih’s idea of creating pieces that will allow audiences to critically think and reflect on what is being is displayed. That being said, it is important that visual aesthetics of data do not overpower the messages we want to send to people, which I feel may be a notable issue to look out for. \nII. A Lesson in Data Visualization: Critiquing a Chart\nNow that we've discussed the foundations of good data visualization in class these last few weeks, I used this assignment to exercise my knowledge in critiquing a graphic in the media. I ended up coming across this article on NBC News, that discussed the opinions of Americans towards race relations at the time. Written by Carrie Dann, NBC/WSJ Poll: Americans Pessimistic on Race Relations (2017) contained two charts demonstrating poll results provided by NBC News and Wall Street Journal. While I feel it is important to keep up with topics the article discussed, I took issue with one of the charts in the article. Pictured below, the chart displays the percentage of poll respondents that perceive race relations in the U.S. as good or bad over time, from 1994-2017.\n\n\n\n\n\nLooking at this chart, there is a significant time period when race relations in the U.S. were viewed positively by Americans overall, and there are clear points where we can see a shift in public opinion. However, it's important to note that the periods of time between each data point are not equal in length, despite being displayed with equal horizontal distance between each other on the x-axis. For example, there is only a 4-month difference between September 9 to January 2010, but there is a 22-month difference from January 2010 to the time of the next set of data points, November 2011. The most notable time jump is between October 1995 to September 2005. Although this seems to be indicated by an unlabeled mark on the x-axis, the shift from a majority of \"total bad\" to \"total good\" participants may not have occurred at a constant rate as displayed on the graph. It may have been more appropriate to indicate a visual break in the main area of the graph as well. \nFurthermore, there are certain parts of the chart that could've been labeled better. For example, it may have been helpful to label the x-axis as displaying month and year - one person could mistake a date such as \"7/13\" as July 13, not July 2013. The chart also displayed the percentage for each data point of the chart. While this could be helpful in context, the size of the chart makes it difficult for viewers to easily process the numbers. The size also caused some overlap between the percentages and lines, making it hard to read.\nAn increase in chart size would not only address the issue of readability, but it could allow for the author to insert smaller captions regarding important events. For example, the article mentions the inauguration of Barack Obama and the verdict of the trial of George Zimmerman. These events, if mentioned in the chart, could give more context to various patterns in the graph. Since the news article is on the shorter side, I can understand why the author may have chosen to publish the graph the way it is. However, I think the chart can overall benefit from a larger size and more context to time differences between its data points.\nIII. Exploring Ascombe.R\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n\nIV. Changing Colors on Fall.R\n\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\ninstall.packages(\"gsubfn\", repos = \"http://cran.us.r-project.org\")\n\n\nThe downloaded binary packages are in\n    /var/folders/bh/qlhrj46x1cg_znsb_xzm3lmm0000gn/T//RtmpxEUnm7/downloaded_packages\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n\nThe downloaded binary packages are in\n    /var/folders/bh/qlhrj46x1cg_znsb_xzm3lmm0000gn/T//RtmpxEUnm7/downloaded_packages\n\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"darkolivegreen3\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes"
  },
  {
    "objectID": "posts/Assignment 2/index.html",
    "href": "posts/Assignment 2/index.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Hello! This assignment contains 3 sections:\n\nReview: Edward Tufte’s “The Future of Data Analysis\nLearning R Through Paul Murrell’s Examples\nApplying R Basics to Happy Planet Index Data\n\nI. Review: Edward Tufte’s “The Future of Data Analysis\nFor this week's assignment, I was able to watch \"The Future of Data Analysis,\" the keynote speech given by Edward Tufte at Microsoft's Machine Learning & Data Science Summit in 2016. As I desire to work in an information-design related role in the future, I believe there are a lot of great takeaways from this talk to consider in my future work.\nOne aspect that I really liked about Tufte's discussion is that he brought in examples of the role of data visualization across various fields. For example, Tufte began with introducing a visualization of vaccination rates during the COVID-19 pandemic. However, he also noted how visualization can be influenced and utilized in other fields. He does not downplay the power of visualization in engaging others, which he says is the key to communicating strong data effectively. At the same time, Tufte also stressed that there are issues in data that must be addressed. With a background in psychology, I nodded along when the replication crisis in the field was mentioned. Tufte's advice - prespecify important factors when dealing with a confirmatory study - falls in line with what I've learned so far about processes of data visualization and guidelines for my field of user research.\nI also very much enjoyed some words of wisdom that Tufte gave during this talk. At one point, Tufte mentions that it's important to have an open mind, but not an empty head. This is similar to a piece of advice I had received - \"hold strong opinions loosely.\" I believe that it's important to critically analyze the world around you and be able to form a viewpoint from the data you can collect. However, as time and other factors shift, more observations can be made that might not fall in line with the patterns that occurred earlier. Therefore, it's important to know what constitutes good data, and how you can apply it to topics that must be addressed.\nII. Learning R Through Paul Murrell’s Examples\nThe following section is the result of observing Paul Murrell’s examples to learn R. Comments/Questions in the code have been addressed below:\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=24)  # Can you change pch? Author: changed to triangle-shaped data points\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols) for each image on 1 page\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=1) # Try different cex value? Author reduced cex  \npoints(x, y2, pch=21, bg=\"cornflowerblue\", cex=1)  # Different background color? Author changed to blue, Reduced cex\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for? This stands for orientation of axis, 1=bottom,2=left,3=top,4=right. In this example, the bottom axis being plotted.\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\", cex=0.5)\n#changed to fit graph\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins\n\nIII. Applying R Basics to Happy Planet Index Data\nIn this section, I tried using several functions to create graphs out of the Happy Planet Index Data. The following is a result of my efforts. Some things I want to note:\n\nHaving to clean the data is an important I had to address that was not taught through observing Murrell’s Graphics!\nIt was difficult to address the variable GDP Per Capita, as it contained missing values. This is something I would like to address in the future.\nMore comments in the code were made for me to use for future reference and areas to gain more knowledge in.\n\n\nlibrary(readxl)\nhpi2019 &lt;- read_excel(\"~/Downloads/happy-planet-index-2006-2020-public-data-set.xlsx\", 2)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n\n#eliminates unneccesary columns\nhpi2019 &lt;- hpi2019[-c(1:7),]\n#eliminates unneccesary row\nhpi2019 &lt;- hpi2019[-c(4)]\n#renames column headers\ncolnames(hpi2019)[1]=\"HPI_rank\"\ncolnames(hpi2019)[2]=\"Country\"\ncolnames(hpi2019)[3]=\"ISO\"\ncolnames(hpi2019)[4]=\"Continent\"\ncolnames(hpi2019)[5]=\"Pop\"\ncolnames(hpi2019)[6]=\"Life_Exp\"\ncolnames(hpi2019)[7]=\"Wellbeing\"\ncolnames(hpi2019)[8]=\"Ecological_Footprint\"\ncolnames(hpi2019)[9]=\"HPI\"\ncolnames(hpi2019)[10]=\"Biocapacity\"\ncolnames(hpi2019)[11]=\"GDP_per_capita\"\n#delete 1st row\nhpi2019 &lt;- hpi2019[-1,]\n\n\n\n#Scatterplot Comparing Life Expectancy and Wellbeing\n#Defining Variables\nx &lt;- hpi2019$Life_Exp\ny &lt;- hpi2019$Wellbeing\n#Setting graph parameters\npar(las=1, mar=c(4, 4, 5, 4))\nplot.new()\nplot.window(xlim=c(50,90), ylim=c(2, 8))\npoints(x, y, pch=21)\npar(col=\"black\", fg=\"black\", col.axis=\"black\")\naxis(1, at=seq(50, 90, 5))\naxis(2, at=seq(2,8,1))\n#plotting a box on just 2 axes\nbox(bty=\"L\")\ntext(60, 7, \"This graph shows a trend...\", cex=0.5)#refer to coordinates you have set\nmtext(\"Life Expectancy (years)\", side=1, line=3)\n#las 0 means sideways label\nmtext(\"Wellbeing\", side=2, line=3, las=0)\nmtext(\"Comparing Life Expectancy vs. Wellbeing in Various Countries\", side=3, line=3,cex=1.25)\n\n\n\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n#Creating a Histogram of Ecological Footprints of Countries\n#changes data to a numeric format\nhpi2019$Ecological_Footprint = as.numeric(as.character(hpi2019$Ecological_Footprint))\nY &lt;- hpi2019$Ecological_Footprint\nY[Y &lt; 0|Y &gt; 20]\n\nnumeric(0)\n\nx&lt;-seq(0,20,1)\ndn &lt;- dnorm(x)\npar(mar=c(4, 4, 4, 4))\nhist(Y, breaks=seq(0, 20,2), ylim=c(0, 0.3), \n     col=\"palevioletred2\", freq=FALSE, main = \"Distribution of Ecological Footprint Among Countries\",xlab=\"Ecological Footprint\")\nlines(density(hpi2019$Ecological_Footprint),lwd =2, col = 'black')\nabline(v = mean(hpi2019$Ecological_Footprint), col=\"darkgreen\",lwd = 1.5)\nabline(v = median(hpi2019$Ecological_Footprint), col=\"gold2\",lwd = 1.5)\n\n#legend\nlegend(15, 0.22, c(\"Mean\", \"Median\",\"Density Line\"), \n       fill = c(\"darkgreen\", \"gold2\", \"black\"), \n       bty=\"n\", cex=0.75)\n\n\n\npar(mar=c(3, 3, 2, 3), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n\n#Creating a Boxplot\n\npar(mar=c(3, 4.1, 2, 0))\n#transforming to numeric data\nhpi2019$Pop = as.numeric(as.character(hpi2019$Pop))\nhpi2019$Life_Exp = as.numeric(as.character(hpi2019$Life_Exp))\nhpi2019$Wellbeing = as.numeric(as.character(hpi2019$Wellbeing))\nhpi2019$HPI_rank = as.numeric(as.character(hpi2019$HPI_rank))\nhpi2019$Continent = as.numeric(as.character(hpi2019$Continent))\n#back to box plot\nlabel= c(\"LA\", \"NAO\",\n                      \"WE\", \"MENA\", \"ARF\", \"SA\", \"EECA\", \"EA\")\nboxplot(hpi2019$Life_Exp ~ hpi2019$Continent,col=\"gray85\" , main = \"Life Expectancy by Region\", xlab=\"Regions\", ylab=\"Life Expectancy\", names=label)\n\n\n\n# Persp Graphic Using HPI, Life Expectancy\n#need increasing order for x and y\ntranspose &lt;- t(hpi2019)\ntranspose &lt;- as.data.frame(transpose)\nreverse2019 &lt;- rev(transpose)\nreverse2019 &lt;-t(reverse2019)\nreverse2019 &lt;- as.data.frame(reverse2019)\nreverse2019$HPI = as.numeric(as.character(reverse2019$HPI))\nx &lt;- reverse2019$HPI\ny &lt;- x\nz&lt;-matrix(reverse2019$Life_Exp,length(y),length(x))\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 15,phi = 30, expand=0.5)\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n#Creating a PieChart of Percentage of Countries by Region \npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.regions &lt;- c((nrow(hpi2019[hpi2019$Continent == '1',])/nrow(hpi2019)),(nrow(hpi2019[hpi2019$Continent == '2',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '3',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '4',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '5',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '6',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '7',])/nrow(hpi2019)), (nrow(hpi2019[hpi2019$Continent == '8',])/nrow(hpi2019)))\nnames(pie.regions) &lt;- c(\"Latin America\", \"N. America & Oceania\",\n                      \"Western Europe\", \"Middle East & North Africa\", \"Africa\", \"South Asia\", \"Eastern Europe & Central Asia\", \"East Asia\")\npie(pie.regions, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n#Would love to learn more on how to order from greatest to least!"
  }
]