[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Assignment Blog",
    "section": "",
    "text": "Hello! This is my 1st Assignment.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nplot(iris)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello there! I am currently a graduate student in The University of Texas at Dallas’ Applied Cognition & Neuroscience Program, specializing in Human-Computer Interaction. With a background in psychology, I have always been interested in using my knowledge to support those in need. When I first learned about user experience (UX), I immediately fell in love with the field’s multidisciplinary nature and evidence-based process.\nI am passionate about creating digital services that are valuable and accessible to diverse user bases. I am a continuous learner and am excited to keep developing my skills through new opportunities in the future! Feel free to reach out and view my resume and/or portfolio.\nWhen I’m not working or studying, you can find me experimenting with songs on my keyboard, volunteering with friends, or exploring new worlds through literature."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Blog!",
    "section": "",
    "text": "Assignment 2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 1\n\n\n\n\n\n\n\ndata visualization\n\n\ngenerative art\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJannelle Navales\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Assignment 1/index.html",
    "href": "posts/Assignment 1/index.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Hello! This assignment contains 4 sections:\n\nExamples of Generative Art\nA Lesson in Data Visualization: Critiquing a Chart\nExploring Ascombe.R\nChanging Colors on Fall.R\n\nI. Examples of Generative Art\nWhat is generative art? \nWhen I first think of this term, I immediately think of the use of artificial intelligence (AI) to create different pieces of art, drawing from other examples from the web or fed into some algorithm. Indeed, generative art often refers to pieces created by an algorithm, but to any autonomous system from various fields, such as statistics or life and physical sciences. Such systems operate as instructions, placing limits on how a work may turn out. However, different outcomes can be generated based on the amount of chance its creator introduces in applying the system. \nOne of the earliest examples of generative art includes George Nees’ Schotter (1968), as seen below. Nees used the ALGOL programming language and introduced random variables to create Schotter. While the top of the artwork begins as standard grid, the comprising squares begin to rotate and move, creating a gradient from order to chaos.  \n\n\n\n\n\n \nNowadays, generative art can be created using a greater variety of programming languages than was offered in 1968. For example, Javascript is one of the most popular languages utilized by artists today. Last spring, I took a web design class and identified several projects that could be classified as generative art. One piece in particular that stood out to me was DT Soulmate Matcher (1968), a piece created by then-MIT graduate student Munus Shih. The platform can be accessed here. A screenshot of the homepage can be found below. \n\n\n\n\n\nDT Soulmate Matcher was partly inspired by Shih’s experiences with classmates in his MFA program, who often wished they could meet more like-minded peers. Furthermore, Shih wanted to investigate the exploratory nature of data visualization and how it could be used to connect audiences. Collecting information from his peers, Shih created a system for how different traits would be visualized in the piece (I.e. patterns, distance between individual profiles) using the library p5.js. He also created an algorithm that allowed for students to be assigned a “soulmate,” or peer that they had most in common with. This system was coded in a way that it could accommodate for future student data, thus altering the initial displays and matches of the page.  \nAnother example of generative art using p5.js includes the following image, created by Ike Stevens. While it may seem the piece might initially appear random at first, it is actually created using Oscar Best Picture Nomination data. The entire process can be further explained in this Medium article, Visualized Best Picture Nominations: Exploring Data-Driven Art (2023). Visual elements of the image were determined by variables such as movie runtime and Rotten Tomatoes review scores. \n\n\n\n\n\nOverall, exploring these pieces demonstrated how generative systems have an important role to play in the future of data visualization. I resonate with Shih’s idea of creating pieces that will allow audiences to critically think and reflect on what is being is displayed. That being said, it is important that visual aesthetics of data do not overpower the messages we want to send to people, which I feel may be a notable issue to look out for. \nII. A Lesson in Data Visualization: Critiquing a Chart\nNow that we've discussed the foundations of good data visualization in class these last few weeks, I used this assignment to exercise my knowledge in critiquing a graphic in the media. I ended up coming across this article on NBC News, that discussed the opinions of Americans towards race relations at the time. Written by Carrie Dann, NBC/WSJ Poll: Americans Pessimistic on Race Relations (2017) contained two charts demonstrating poll results provided by NBC News and Wall Street Journal. While I feel it is important to keep up with topics the article discussed, I took issue with one of the charts in the article. Pictured below, the chart displays the percentage of poll respondents that perceive race relations in the U.S. as good or bad over time, from 1994-2017.\n\n\n\n\n\nLooking at this chart, there is a significant time period when race relations in the U.S. were viewed positively by Americans overall, and there are clear points where we can see a shift in public opinion. However, it's important to note that the periods of time between each data point are not equal in length, despite being displayed with equal horizontal distance between each other on the x-axis. For example, there is only a 4-month difference between September 9 to January 2010, but there is a 22-month difference from January 2010 to the time of the next set of data points, November 2011. The most notable time jump is between October 1995 to September 2005. Although this seems to be indicated by an unlabeled mark on the x-axis, the shift from a majority of \"total bad\" to \"total good\" participants may not have occurred at a constant rate as displayed on the graph. It may have been more appropriate to indicate a visual break in the main area of the graph as well. \nFurthermore, there are certain parts of the chart that could've been labeled better. For example, it may have been helpful to label the x-axis as displaying month and year - one person could mistake a date such as \"7/13\" as July 13, not July 2013. The chart also displayed the percentage for each data point of the chart. While this could be helpful in context, the size of the chart makes it difficult for viewers to easily process the numbers. The size also caused some overlap between the percentages and lines, making it hard to read.\nAn increase in chart size would not only address the issue of readability, but it could allow for the author to insert smaller captions regarding important events. For example, the article mentions the inauguration of Barack Obama and the verdict of the trial of George Zimmerman. These events, if mentioned in the chart, could give more context to various patterns in the graph. Since the news article is on the shorter side, I can understand why the author may have chosen to publish the graph the way it is. However, I think the chart can overall benefit from a larger size and more context to time differences between its data points.\nIII. Exploring Ascombe.R\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n\nIV. Changing Colors on Fall.R\n\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\ninstall.packages(\"gsubfn\", repos = \"http://cran.us.r-project.org\")\n\n\nThe downloaded binary packages are in\n    /var/folders/bh/qlhrj46x1cg_znsb_xzm3lmm0000gn/T//RtmpxEUnm7/downloaded_packages\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n\nThe downloaded binary packages are in\n    /var/folders/bh/qlhrj46x1cg_znsb_xzm3lmm0000gn/T//RtmpxEUnm7/downloaded_packages\n\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"darkolivegreen3\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes"
  },
  {
    "objectID": "posts/Assignment 2/index.html",
    "href": "posts/Assignment 2/index.html",
    "title": "Assignment 2",
    "section": "",
    "text": "This assignment is currently a work in progress!"
  }
]