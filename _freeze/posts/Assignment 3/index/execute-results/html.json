{
  "hash": "d323737b370b41a2d955a303b05bae59",
  "result": {
    "markdown": "---\ntitle: \"Assignment 3\"\nauthor: \"Jannelle Navales\"\ndate: \"2023-09-27\"\ncategories: [data analysis, big data, overfitting, overparameterization, tidyverse, r, scatterplot, ggplot2]\n---\n\n\nHello! This assignment contains 3 sections:\n\n1.  Review: The Parable of Google Flu: Traps in Big Data Analysis\n\n2.  Review: Wickham - Data Visualization and Data Analysis (EMBL)\n\n3.  Modifying Anscombe.R using base and ggplot2 graphics\n\n**I. Review: The Parable of Google Flu: Traps in Big Data Analysis**\n\nBack in 2013, one may have been familiar with Google Flu Trends (GFT), a website created by Google to keep track of influenza case estimates. At the time, the platform was heralded as being an innovative use of big data. But while the GFT had good intentions in trying to predict similar trends in data with other organizations, the GFT produced various errors than expected. What happened?\n\nThe GFT is an example of what some people call the big data analytics pitfall - the idea that using big data can replace traditional methods of data collection and analysis. In traditional studies, factors such as construct validity and reliability are given considerable attention, as to make sure data represents concepts as intended. And these factors are sometimes overlooked when we utilize big data. For example, the GFT considered the search term \"high school basketball\" to be indicative of flu incidences. The authors hypothesize that the platform conflated \"winter\" words with flu-case indicative words (for context, influenza season and high school basketball seasons typically occur in winter. Similar cases can be caused by a concept known as overfitting, for which occurs when a model trained from a training set does not give accurate predictions for new data. This sometimes causes the inclusion of more items than intended. Overfitting can further be explained by the practice of overparameterization, when the number of parameters used exceeds the size of the training dataset. It should be noted that this is a common practice in big data analysis.\n\nFinally, the article points out what I find the fatal flaw of the GFT; it relies on an algorithm that is meant to drive revenue and business needs, which can differ from what the search patterns the general population utilize for the sake of simply treating the flu. This demonstrates the need for data scientists to pay closer attention to transparency, constructs represented by algorithms, and recognize big data's weaknesses.\n\n**II. Review: Wickham - Data Visualization and Data Analysis (EMBL)**\n\nIn Hadley Wickham's keynote speech at EMBL, Wickham goes over various visualization techniques, specifically within the tidyverse package for R, and advocates for how code can be quite useful for visualization. At first, he starts out with how the tidyverse packages came to be. The need to more easily modify the visual aspects of graphics helped spur the creation of ggplot, which has since evolved into ggplot2. But as Wickham noted and in my experience as well, creating the visualization is not the most difficult part of the process. Formatting the data correctly, however, can be lengthy, which inspired to creation of reshape, now in its current iteration of tidyr. The dplyr and purrr packages were further created to help aggregate data correctly. And thus, came the creation of the tidyverse.\n\nThrough walking through a visualization example using ggplot2, one can see how such packages were created with the grammar of graphics in mind. For example, the geom_point() function can easily control the aesthetics of different visual components. Another function helps control the scale of the data, another noted step in the GoG theory. Wickham claims that this structure, which enables easy modification to orthogonal, independent components, greatly increases users' ability to explore data and discover greater connections.\n\nWickham also acknowledges the advantages coding may have in data visualization over other technologies, such as the spreadsheet interfaces of Tableau and Excel. It is said that non-coding platforms have a much nicer learning curve and can be less time-consuming than that of coding platforms. However, much of these platforms have default settings that do not communicate data as effectively as intended. I also personally find it more difficult to replicate the formatting of data presentation (i.e. formulas) for other data sets when using such tools. Meanwhile, coding allows for easy reproduction, a wider range of customization, and a greater ability for users to explore the insights data can offer.\n\n```         \n```\n\n**III. Modifying Anscombe.R using base and ggplot2 graphics**\n\nAs shown in Assignment 1, Ascombe (1973) displays different graphs that are shown to have the same r-coefficient via linear regression analysis. The problem that this data showcases is that simply knowing the r-value is NOT enough to have a complete picture of the data. Graph 1 (Top Row, Left) does not have any problems - the data point distribution strongly suggests a positive, linear relationship. In contrast, Graph 2 (Top Row, Right) suggests a non-linear fit model. The bottom row graphs also hint at the effect of an outlier on its coefficient correlation.\n\nThe graphs from Ascombe can be seen below. Note that these graphs are modified from the originals, as I played around with the code to experiment with changing colors, line types, plotting characters and fonts without packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nsummary(anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n```\n:::\n\n```{.r .cell-code}\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n```\n:::\n\n```{.r .cell-code}\n# Create four model objects\nlm1 <- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,\tAdjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n```\n:::\n\n```{.r .cell-code}\nlm2 <- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,\tAdjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n```\n:::\n\n```{.r .cell-code}\nlm3 <- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,\tAdjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n```\n:::\n\n```{.r .cell-code}\nlm4 <- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,\tAdjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n```\n:::\n\n```{.r .cell-code}\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n\n```{.r .cell-code}\n## Fancy version (per help file)\n\nff <- y ~ x\nmods <- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] <- as.name(paste0(\"y\", i))\n  ##      ff[[3]] <- as.name(paste0(\"x\", i))\n  mods[[i]] <- lmi <- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nsapply(mods, coef)  # Note the use of this function\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n```\n:::\n\n```{.r .cell-code}\nlapply(mods, function(fm) coef(summary(fm)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$lm1\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n```\n:::\n\n```{.r .cell-code}\n# Preparing for the plots\nop <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0), family=\"Times New Roman\")\n#family in par -> changes fonts in graph\n#for Windows users, you can also use windowsFont(), unfortunately, I have a Mac...\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"midnightblue\",type=\"p\", pch = 23, bg = \"lightseagreen\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"firebrick\", lwd=2, lty=2)\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-5.png){width=672}\n:::\n\n```{.r .cell-code}\npar(op)\n#plotting character can be changed using pch\n#type indicates whether points (p), lines(l), or both(b) should be plotted\n#lty changes the type of line (dotted, semi dotted)\n```\n:::\n\n\nNext, I wanted to see how I could change the plots using ggplot2 (the tidyverse package). These are the results of my experimentation below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\n\n\n\n## Simple version\nggplot(anscombe, aes(x=x1, y=y1))+geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## geom_point changes graph characteristics\n## can change size, shape, color\nggplot(anscombe, aes(x=x1, y=y1)) + geom_point(size=2, shape=5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\n## if you wnat to just change size of POINTS, use (aes(size=1)) within geom_point\n## if you ever wanted to label points, use geom_text(label=rownames(anscombe))\n\n##plot regression line using geom_smooth\n#specify for linear with method=lm, delete range with se=FALSE\n\nggplot(anscombe, aes(x=x1, y=y1)) + geom_point(size=2, shape=17,color=\"lightseagreen\") + geom_smooth(method=lm, se=FALSE,color=\"firebrick3\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n\n```{.r .cell-code}\n#you can change the line type and color within geom_smooth as well\nggplot(anscombe, aes(x=x2, y=y2)) + geom_point(size=2, shape=17,color=\"mediumpurple3\") + geom_smooth(method=lm, se=FALSE,color=\"darkorange4\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-4.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(anscombe, aes(x=x3, y=y3)) + geom_point(size=2, shape=17,color=\"palegreen3\") + geom_smooth(method=lm, se=FALSE,color=\"deeppink4\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-5.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(anscombe, aes(x=x4, y=y4)) + geom_point(size=2, shape=17,color=\"violetred1\") + geom_smooth(method=lm, se=FALSE,color=\"hotpink4\",linetype=\"dashed\")+xlim(3,19)+ylim(3,13)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-6.png){width=672}\n:::\n\n```{.r .cell-code}\n#doing with this with other graphs\n```\n:::\n\n\n**IV. Running the Pre-Hackathon Data**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\nView(owidall)\n#Deselect cases/rows with OWID\n#grepl() searches for matches in characters/sequences of characters in string\nowidall = owidall[!grepl(\"^OWID\",owidall$iso_code), ]\nView(owidall)\n#Subset by continent: Europe\nowideu = subset(owidall, continent==\"Europe\")\nView(owideu)\n\n# Europe data\ny = owideu$new_deaths\n#as.Date converts character objects to date objects\nx = as.Date(owideu$date)\n#xaxt gives the style, 'n' means no scale\nplot(x,y, pch=20, col=\"#E7298A\", cex = .5, xaxt='n', xlab = \"Date\", ylab = \"COVID Deaths in Europe (Daily)\")\n#note for format - %Y-%m gives the dates in year/month format\n#tick marks - in this example we do not display\naxis(1, x, format(x, \"%Y-%m\"), cex.axis = .7, las = 3 , gap.axis =1.5, tick = FALSE)\nidentify(x,y,owideu$location, ps=8, atpen=TRUE) # Manually identify cases by mouse click\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ninteger(0)\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}